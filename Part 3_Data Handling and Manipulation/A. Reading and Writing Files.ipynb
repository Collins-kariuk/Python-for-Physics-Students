{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effectively managing large datasets is a vital skill in scientific computing, as encountering datasets that exceed manageable sizes is a common occurrence. From reading experimental measurements and analyzing extensive datasets to exporting results, Python offers a flexible and powerful set of tools for file handling. This section explores techniques for reading, processing, and writing files in commonly used formats, with Physics-inspired examples to make the concepts practical and accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Saving CSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV files are a widely used format for storing structured data in a tabular form, where rows represent individual records and columns represent different variables. The values are separated by commas, hence the name: Comma-Separated Values.\n",
    "\n",
    "In practice, you may encounter CSV files when handling experimental measurements, simulation outputs, or datasets from online sources. Their simplicity and broad compatibility make them a popular choice for data storage.\n",
    "\n",
    "Python provides two main tools for working with CSV files:\n",
    "\n",
    "- the built-in csv module and\n",
    "- the Pandas library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the csv Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A module, simply put, is a file containing Python code that can include functions, classes, and variables. A Python file qualifies as a module if it contains code that can be executed. For example, the csv module offers convenient functionality for both reading and writing CSV files through straightforward methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'Position']\n",
      "['0', '0']\n",
      "['1', '5']\n",
      "['2', '20']\n",
      "['3', '45']\n",
      "['4', '80']\n",
      "['5', '100']\n"
     ]
    }
   ],
   "source": [
    "import csv # import the csv module\n",
    "\n",
    "# open and read a CSV file\n",
    "with open('data.csv', mode = 'r', encoding = \"utf-8-sig\") as file:\n",
    "    \"\"\"\n",
    "    mode = 'r' is for reading\n",
    "    encoding = \"utf-8-sig\" is for encoding the file. This is\n",
    "    important for files that have special characters and is\n",
    "    good practice to include it.\n",
    "    \"\"\"\n",
    "\n",
    "    reader = csv.reader(file) # create a reader object\n",
    "    for row in reader: # iterate over the rows in the file\n",
    "        print(row)\n",
    "        # print(row[0], row[1], row[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `csv.reader()` function reads the contents of a CSV file and returns an iterable object, allowing you to loop through the data row by row. In the example above, it demonstrates how to read a CSV file named data.csv and print its contents. Essentially, the `csv.reader()` function takes a file object as an argument and returns a reader object (similar to a list of lists) that can be iterated over to access the data.\n",
    "\n",
    "To read a CSV file in Python, follow these steps:\n",
    "1. Open the file using the `open()` function in the appropriate mode (e.g., 'r' for reading), and use the `with` statement to ensure the file is automatically closed after use.\n",
    "2. Create a reader object using the `csv.reader()` function to process the fileâ€™s contents.\n",
    "3. Iterate over the reader object to access the data row by row.\n",
    "\n",
    "The `with` statement is essential when working with files, as it ensures the file is properly closed once the code block is executed. Without it, the file may remain open, potentially causing issues like memory leaks. Using the `with` statement is a best practice for file handling in Python.\n",
    "\n",
    "This approach allows you to handle simple CSV files without needing external libraries. However, for more advanced operations, the Pandas library provides a more powerful and flexible option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write data to a CSV file using the `csv.writer()` function, which creates a writer object to handle the output. This writer object provides methods like `writerow()` for writing a single row and `writerows()` for writing multiple rows at once. For example, imagine you have a list of experimental measurements that you want to save to a CSV file. You might start by adding a header row to label the columns, which can be done by writing a list of column names before the data. Then, you can write the measurements to a file, such as results.csv, using the `csv.writer()` function to organize and save the data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "new_data = [[\"Time\", \"Position\"], [0, 0], [1, 5], [2, 20]] # data to write to the file\n",
    "\n",
    "with open(\"output.csv\", mode=\"w\", newline=\"\") as file: # open the file\n",
    "    csv_writer = csv.writer(file) # create a writer object\n",
    "    csv_writer.writerows(new_data) # write the data to the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scenario above, the `csv.writer()` function takes a file object and an optional `delimiter` argument (defaulting to a comma) to specify the character used to separate values. The `writerow()` method writes a single row to the CSV file, while the `writerows()` method writes multiple rows at once. By following these steps, you can write data to a CSV file in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pandas to handle CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to work with CSV files in Python is by using the Pandas library, which provides powerful tools and data structures for data manipulation. Built on top of NumPy, Pandas introduces data structures like Series and DataFrame, making it ideal for handling structured data. Itâ€™s a go-to library for data scientists and researchers, offering a wide range of functions for seamlessly reading, writing, and processing data in various formats, including CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a CSV File into a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pandas, a DataFrame is a two-dimensional, tabular data structure similar to a spreadsheet or SQL table. It consists of rows and columns, where:\n",
    "- Rows represent individual records or observations.\n",
    "- Columns represent variables or features.\n",
    "\n",
    "DataFrames can handle diverse data types and allow easy data manipulation, such as filtering, aggregation, and transformations. They are the core data structure in Pandas, making it simple to clean, analyze, and visualize structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time  Position\n",
      "0     0         0\n",
      "1     1         5\n",
      "2     2        20\n",
      "3     3        45\n",
      "4     4        80\n"
     ]
    }
   ],
   "source": [
    "# %pip install pandas # install the pandas library, just in case you don't have it\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data.csv\") # Load data into a DataFrame\n",
    "print(df.head()) # Display the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing DataFrames to a CSV File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once youâ€™ve processed or analyzed your data using Pandas, you might need to save the results for future use or to share with others. Pandas makes this simple with the `to_csv()` method, which writes a DataFrame to a CSV file in just one line of code. This ensures your data is stored in a structured, widely compatible format thatâ€™s easy to use with other tools.\n",
    "\n",
    "The `to_csv()` method also provides various parameters for customizing the output, such as specifying the delimiter, including or excluding the header, and controlling whether the DataFrameâ€™s index is written to the file.\n",
    "\n",
    "\n",
    "When you execute the code above and open the generated CSV file, youâ€™ll notice that it includes both the data and the index along with column labels. By default, these are included in the output, but you can exclude them by setting the parameters `index=False` and/or `header=False` in the `to_csv()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ A delimiter is a character or sequence of characters used to separate values in a data file. In CSV files, the default delimiter is a comma (,), but other characters like tabs (\\t), semicolons (;), or spaces can also be used, depending on the file format. Delimiters define how data is structured and parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a DataFrame to a CSV file\n",
    "df.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Large Datasets Efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some research situations, you may encounter large datasets that exceed the available memory, making it challenging to process the data efficiently. In such cases, you need to adopt strategies to handle large datasets effectively without running into memory errors or performance issues. Python provides efficient methods to process such data incrementally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Data with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking processes data in smaller portions, avoiding memory overflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read a large CSV file in chunks\n",
    "chunk_size = 1000\n",
    "for chunk in pd.read_csv(\"large_data.csv\", chunksize=chunk_size):\n",
    "    print(chunk.shape) # Process each chunk as a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is particularly useful for analyzing large datasets, such as experimental data with millions of rows, where loading the entire dataset into memory at once may not be feasible. While `large_data.csv` in this example doesnâ€™t have millions of rows, you get the idea.\n",
    "\n",
    "Thereâ€™s plenty more to explore on this topic! If youâ€™re interested in learning more, check out the following resources:\n",
    "\n",
    "- [YouTube Video](https://www.youtube.com/watch?v=xtFo1IiZqzM)\n",
    "- [Pandas Documentation on Scaling to Large Datasets](https://pandas.pydata.org/docs/user_guide/scale.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Brief Overview of Other File Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from CSV files, you may encounter various other file formats in scientific computing, each with its unique characteristics and use cases. Hereâ€™s a brief overview of some common file formats and their applications:\n",
    "\n",
    "- **Excel Files (.xlsx):** Excel files are widely used for storing tabular data, formulas, and charts. You can read and write Excel files in Python using libraries like Pandas, openpyxl, and xlrd.\n",
    "- **HDF5 Files (.h5):** HDF5 files are ideal for storing large datasets with complex structures. The h5py library provides tools for reading and writing HDF5 files in Python.\n",
    "- **JSON Files (.json):** JSON files are commonly used for storing structured data in a human-readable format. Pythonâ€™s built-in json module allows you to work with JSON files easily.\n",
    "\n",
    "Each file format has its advantages and limitations, so choosing the right format depends on your specific requirements and use case. By understanding the characteristics of different file formats, you can effectively manage and process data in scientific computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with JSON Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON (JavaScript Object Notation) is widely used for structured data, such as simulation configurations or metadata. Itâ€™s a lightweight, human-readable format thatâ€™s easy to parse and generate, making it a popular choice for data exchange between systems. Pythonâ€™s built-in json module provides functions for encoding and decoding JSON data, allowing you to read and write JSON files effortlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experiments': [{'id': 1, 'name': 'Projectile Motion', 'date': '2023-11-20', 'results': {'initial_velocity': 20, 'angle': 45, 'max_height': 10.2, 'time_of_flight': 2.8, 'range': 28.4}}, {'id': 2, 'name': 'Simple Harmonic Motion', 'date': '2023-11-21', 'results': {'mass': 0.5, 'spring_constant': 10, 'amplitude': 0.2, 'period': 1.4, 'frequency': 0.71}}, {'id': 3, 'name': 'Free Fall', 'date': '2023-11-22', 'results': {'height': 15, 'time': 1.75, 'final_velocity': 17.1, 'acceleration_due_to_gravity': 9.8}}]}\n"
     ]
    }
   ],
   "source": [
    "# Reading JSON files\n",
    "import json\n",
    "\n",
    "# Load data from a JSON file\n",
    "with open(\"data.json\", mode=\"r\") as file:\n",
    "    data = json.load(file)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the explanation of the structure of a JSON file:\n",
    "\n",
    "- **Root Object:** Contains a single key, \"experiments\", which is an array of individual experiment objects.\n",
    "- **Experiment Object:**\n",
    "  - **id:** Unique identifier for the experiment.\n",
    "  - **name:** Name of the experiment.\n",
    "  - **date:** Date the experiment was conducted.\n",
    "  - **results:** A nested object containing relevant parameters and outcomes of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to a JSON file\n",
    "with open(\"output.json\", mode=\"w\") as file:\n",
    "    json.dump(data, file, indent=4) # indent for pretty printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Tips for File Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, here are some practical tips for effective file handling in Python:\n",
    "\n",
    "- **Use Context Managers:** Always use the `with` statement when working with files to ensure they are properly closed after use.\n",
    "- **Choose the Right File Format:** Select the appropriate file format based on your data requirements and use case.\n",
    "- **Leverage Libraries:** Take advantage of libraries like Pandas, NumPy, and h5py for efficient data processing and storage.\n",
    "- **Optimize Memory Usage:** When working with large datasets, use chunking or incremental processing to avoid memory issues.\n",
    "- **Documentation:** Add comments and docstrings to your code to explain the purpose of each file operation and make it easier to understand and maintain.\n",
    "- **Error Handling:** Implement error handling to manage exceptions and ensure robust file handling in your code. I know we have not handled this but take a look at this [Python Try Except](https://www.w3schools.com/python/gloss_python_try_except.asp) for more information. Errors are common in file handling, so itâ€™s essential to handle them gracefully. An example:\n",
    "\n",
    "```python\n",
    "# The try block attempts to execute the file operation\n",
    "try:\n",
    "    with open(\"data.csv\", mode=\"r\") as file:  # Open the file in read mode\n",
    "        data = file.read()  # Read the file contents\n",
    "\n",
    "except FileNotFoundError:  # Handle the case where the file is not found\n",
    "    print(\"The file was not found.\")  # Print an error message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics-Inspired Example: Analyzing Experimental Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
